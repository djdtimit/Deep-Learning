{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djdtimit/Deep-Learning/blob/master/convolutional_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGaxmXYExvj4",
        "colab_type": "text"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EErXOVCExzfu",
        "colab_type": "text"
      },
      "source": [
        "## Computer Vision Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP2xbzRax3K5",
        "colab_type": "text"
      },
      "source": [
        "- Image Classification: Cat or not?\n",
        "- Object Detection: Finding object in image and adding bounding box\n",
        "- Neural Style Transfer: Making Picture of two images in the style of one image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YZ9LVP1zEQC",
        "colab_type": "text"
      },
      "source": [
        "## Difficulties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0Br73NDzEMr",
        "colab_type": "text"
      },
      "source": [
        "- With large images as input, there will be a lot of parameters in an fully connected architecture -> overfitting and high computational requirements -> solution: convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9jQ641tpLZp",
        "colab_type": "text"
      },
      "source": [
        "## Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfF6I0TEpLTF",
        "colab_type": "text"
      },
      "source": [
        "- Convnets are the best type of machine-learning models for computer-vision\n",
        "tasks. It’s possible to train one from scratch even on a very small dataset, with\n",
        "decent results.\n",
        "- On a small dataset, overfitting will be the main issue. Data augmentation is a\n",
        "powerful way to fight overfitting when you’re working with image data.\n",
        "- It’s easy to reuse an existing convnet on a new dataset via feature extraction.\n",
        "This is a valuable technique for working with small image datasets.\n",
        "- As a complement to feature extraction, you can use fine-tuning, which adapts to\n",
        "a new problem some of the representations previously learned by an existing\n",
        "model. This pushes performance a bit further.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xfsHw3wzEJY",
        "colab_type": "text"
      },
      "source": [
        "## Idea behind Convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGwXyLYRzEGJ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1i127PLBw0d-nmwkkVWkt7E4_senC6QGd\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GB1MggozEDK",
        "colab_type": "text"
      },
      "source": [
        "First detect edges than other parts of the face and in the end the whole face => low level features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5nz3I_5zEAM",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional operation for vertical edge detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loo7RPdPzD88",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=10-0JTDw55R-I4tedIrdXXumJzsq3h9xX\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlJTj5LUzD58",
        "colab_type": "text"
      },
      "source": [
        "## Why will applying this filter detect vertical edges?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaKMYR8NzD2t",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Nu5dNjrrBtw2oqdv5Q7BclpC2q0t5wQl\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SahUJpbzDzh",
        "colab_type": "text"
      },
      "source": [
        "## Learning to detect edges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ant6ua1azDwQ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1OhqOonrQni3gNDyYtfXczIn1lRNp3xpB\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayhjrJ7TzDtb",
        "colab_type": "text"
      },
      "source": [
        "treat the numbers in the filter as parameters. These parameters will be learned from the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dTgnTvTzDqQ",
        "colab_type": "text"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm413qbBzDnH",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1o3Iv4jgdGm8vtJeQwdbwR8Lwo1Q17bWa\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBS-z0PZzDkG",
        "colab_type": "text"
      },
      "source": [
        "applying the convolution operation leads to two problems:\n",
        "\n",
        "- the output shrinks\n",
        "- the filter is not often applied to the edges such that a lot of information from the edges will be thrown away\n",
        "\n",
        "solution: padding\n",
        "\n",
        "padding means to extend the input with new edges filled with zeros. This will solve the problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1awxD6JdzDhV",
        "colab_type": "text"
      },
      "source": [
        "## Valid and Same convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIJS2EmgzDeJ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1U7-iTVxIjszgFqrdL_k4ZFzbFxg6Q9o2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8lr_dG9zDbO",
        "colab_type": "text"
      },
      "source": [
        "valid convolution means no padding and the output shrinks\n",
        "\n",
        "same convolution means using padding such that the output size will not shrink. In this case p must be chosen according to the formula and f needs to be odd. f is usually odd because in this case the filter has a central position.\n",
        "\n",
        "valid: only valid window locations will be used\n",
        "\n",
        "same: pad in such a way as to have an output with the same width and height as the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJOGKUQhzDYB",
        "colab_type": "text"
      },
      "source": [
        "## strided convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoQn3JuIzDVF",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ZeAjCZlOkcrdqHnMpWxoYr3krrKj4PWH\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgN9mjNpzDR_",
        "colab_type": "text"
      },
      "source": [
        "stepping over by two steps instead of one when applying convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO5BZhWczDO6",
        "colab_type": "text"
      },
      "source": [
        "## Convolution on volumes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOTdBmAFzDL8",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1klhJJ2g-sMxM79OU0MbhuWVrTIEfPCia\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-Fb1J0lzDI8",
        "colab_type": "text"
      },
      "source": [
        "the third dimension (=channel) should be the same between the input and the filter. Apply convolution channel by channel and add the results to get a result with a single channel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uly4WzknzDF_",
        "colab_type": "text"
      },
      "source": [
        "## Multiple Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpniOl4szDC9",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1sgNcYB3c4rUpLtLdzIIjs27a1M-HUPge\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdDQxt3AzC_5",
        "colab_type": "text"
      },
      "source": [
        "applying several filters for different kind of detection (vertical or horizontal edge detection) will result in an output with the number of channels equal to the number of filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_4mcm_SzC8-",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1W9NPToxU-5nByL29KT7asQ54zmZH47VK\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhyE-duczC6G",
        "colab_type": "text"
      },
      "source": [
        "to the output add bias and add activation function. Afterwards use this as the input to the next layer\n",
        "\n",
        "the number of parameters to learn are included in the filter and in the bias\n",
        "\n",
        "the number of parameters are independent of the size of the image. this is different to a fully connected layer and this architecture is not as prone to overfitting as a fully connected architecture (see above)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys02s2LLzC2g",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1xmVtbkrKaZV0WACM-Vyb1qfYxobNyjS6\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StLPYUhwoIqa",
        "colab_type": "text"
      },
      "source": [
        "The striding parameter s results in fast reduction of the first and second output dimension.\n",
        "The first and second dimension will for s = 0 always shrink by 2. If s is applied this is also the case but the result is divided by s and the floor is pplied to the result\n",
        "\n",
        "After the final convolution layer in this example are 1960 features learned. These features will be flattenend and will be fed into an activation function to generate the predictions.\n",
        "\n",
        "Convolution is typically done with 3 x 3 windows and stride = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hoa3bPn8oInY",
        "colab_type": "text"
      },
      "source": [
        "## Pooling Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRWr4_60oIj_",
        "colab_type": "text"
      },
      "source": [
        "- to reduce the size of the representation\n",
        "- to speed up computation\n",
        "- to make features more robust"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00vjdFkboIg1",
        "colab_type": "text"
      },
      "source": [
        "### Max Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OFpvm7BoId3",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1uxRFT1YVxGD2prQRo0DNLf4Tr_mxyK_F\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JeC0PVYoIaZ",
        "colab_type": "text"
      },
      "source": [
        "f = dimension of filter \n",
        "s = stride\n",
        "=> apply 3x3 filter to input feature map with stride of 1 and return max\n",
        "\n",
        "The input feature map channel size is preserved\n",
        "\n",
        "So what the max operation does is a lots of features detected anywhere, and one of these quadrants , it then remains preserved in the output of max pooling.\n",
        "\n",
        "max pooling is usually done with 2 x 2 windows and stride 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNcMueFGoIWn",
        "colab_type": "text"
      },
      "source": [
        "### average pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66emZaIoRVo_",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1vmStEcB_lVDM4KXLdUN36XKaAeXkv1Bk\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibn0ZR7aRVmV",
        "colab_type": "text"
      },
      "source": [
        "Same idea as max pooling but taking the average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1fL2bONRVjC",
        "colab_type": "text"
      },
      "source": [
        "For pooling layers f and s are the hyperparameters. There are no parameters to learn in this layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGDLHkc8RVgJ",
        "colab_type": "text"
      },
      "source": [
        "## Neural network example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BERrMmbYRVc_",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1hy4m5F06EePJunVqpVtP_ixWtDXTuDSN\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUV4kxseRVaF",
        "colab_type": "text"
      },
      "source": [
        "formula: $\\lfloor\\frac{n+2p-f}{s} + 1\\rfloor$\n",
        "Here: n = 32 (n_c = 3), p = 0, f = 5, s = 1 => 28\n",
        "\n",
        "convention: conv + max-pooling = one layer since max pooling contains no parameter to learn\n",
        "\n",
        "common pattern: n_c, n_w decreases, n_c increases\n",
        "\n",
        "n_c in CONV1 and POOL1 should be 8 (see below)\n",
        "\n",
        "Convolutions operate over 3D tensors, called feature maps.\n",
        "The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVJb-YY_RVWx",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1f-6XGKXZNXliRA7yS8pUBF6rYLCF1QBY\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSIr1mmbRVTk",
        "colab_type": "text"
      },
      "source": [
        "activation size = n_w x n_h x n_c\n",
        "\n",
        "number of parameters = (f x f x n_c_previous_layer + bias) x n_c_current_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEVNP3LfRVQw",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1r70_JlEs9hQ5J8Ovcj2zGv6SzQrK89Ru\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6xFrZ4jRVNc",
        "colab_type": "text"
      },
      "source": [
        "Parameter sharing: The learned filter is useful for different parts of the image\n",
        "\n",
        "Sparsity of connections: convolution and pooling reduces the output size of a layer\n",
        "\n",
        "The fundamental difference between a densely connectd layer and a convolution layer:\n",
        "\n",
        "- Dense layers learn global patterns in their input feature space\n",
        "\n",
        "- Convolution layers learn local patterns \n",
        "\n",
        "The patterns convnets learns are translation invariant:\n",
        "\n",
        "- a convnet can regognize learned patterns anywhere\n",
        "\n",
        "- densely connected network would have to learn the patterns anew if it appeared at a new location\n",
        "\n",
        "=> convnets need fewer training examples\n",
        "\n",
        "Convnets can learn spatial hierarchies of patterns:\n",
        "\n",
        "A first convolution layer will learn small local patterns such as edges, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn increasingly complex and abstract visual concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obOfsvEjRVKQ",
        "colab_type": "text"
      },
      "source": [
        "## Initiating a small convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03Y-fWYlxoOB",
        "colab_type": "code",
        "outputId": "0b86a0fb-e599-4dc7-c62e-7ceb170f920e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import models"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c053oAzigZeh",
        "colab_type": "code",
        "outputId": "43b2b3de-278d-4cf2-9ac6-a5031469b9db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktD2D7BFiB77",
        "colab_type": "code",
        "outputId": "04a7ebdf-0b24-4a87-89f9-8d0f1bf57b7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meCxBJxtkwVA",
        "colab_type": "text"
      },
      "source": [
        "number of parameters = (f x f x n_c_previous_layer + bias) x n_c_current_layer => (3 x 3 x 1 + 1) x 32 = 320\n",
        "\n",
        "(3 x 3 x 32 +1) x 64 = 18496\n",
        "\n",
        "(3 x 3 x 64 +1) x 64 = 36928"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND0b-2HQmjWz",
        "colab_type": "text"
      },
      "source": [
        "The output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and high dimension goes down as mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1KPJbZXnRBX",
        "colab_type": "text"
      },
      "source": [
        "## Adding a classifier on top of the convnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGiuoDHsissd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(units=64,activation='relu'))\n",
        "model.add(layers.Dense(10,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YitryVrYnz3t",
        "colab_type": "code",
        "outputId": "ff0a4c95-39e6-47df-b0e0-9f8cd198b3e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e65MDcz8oNxh",
        "colab_type": "text"
      },
      "source": [
        "The flatten 1 layer returns 3 x 3 x 64 = 576 inputs \n",
        "\n",
        "The dense_1 layer contains 576 * 64 + 64 = 36928 parameters\n",
        "\n",
        "The dense_2 layer contains 65 x 10 + 10 = 650 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zQRrl0EqmXE",
        "colab_type": "text"
      },
      "source": [
        "## Training the convnet on MNIST images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEocIWJPn3U0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2THpNcDqz-O",
        "colab_type": "code",
        "outputId": "c0e34c79-ba67-48f6-d1b5-38d6f835651a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUcnLNhOsFFk",
        "colab_type": "code",
        "outputId": "2cdaffbf-76a3-4162-9e6b-71a6c2e4f850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovfXObeqsEiQ",
        "colab_type": "code",
        "outputId": "9f5e301c-45be-4f20-851c-9b959c4d9388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_kusqgkrG6_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images.reshape((60000,28,28,1))\n",
        "train_images = train_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hg9nv5qrMrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_images = test_images.reshape((10000,28,28,1))\n",
        "test_images = test_images.astype('float32') / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lQJeG_hs2sP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTz56r59tITE",
        "colab_type": "code",
        "outputId": "a8984293-79b5-404e-f23f-7d77daa43cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKxY_MRAtYZq",
        "colab_type": "code",
        "outputId": "9f8a96d1-672c-4566-d14f-5f0246bf2422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "model.fit(x=train_images, y=train_labels, batch_size=64,epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.1807 - acc: 0.9432\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.0464 - acc: 0.9856\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.0319 - acc: 0.9902\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 68us/step - loss: 0.0252 - acc: 0.9919\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.0194 - acc: 0.9943\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f909c3aaa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcO-5E8evCiJ",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2RJTKacudlX",
        "colab_type": "code",
        "outputId": "8869cbb2-4789-4f12-9eeb-4384cbdce98f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x=test_images, y=test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 63us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB4AhWJFvaaY",
        "colab_type": "code",
        "outputId": "567e5f8f-2336-4069-911b-946a7b975ebf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9921"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-4b0TrQvdmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}